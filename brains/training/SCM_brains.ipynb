{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f042f017-1827-4760-8314-0aec855fbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lz4\n",
    "import nibabel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.nn import pyro_method, DenseNN\n",
    "from pyro.distributions.conditional import ConditionalTransformModule\n",
    "from pyro.distributions import RelaxedBernoulliStraightThrough\n",
    "import sys\n",
    "sys.path.insert(0,\"../data/\")\n",
    "import torch\n",
    "from loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "109db419-2233-4102-ac35-1d90ce9e8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if (type(m) == nn.Conv3d or\n",
    "        type(m) == nn.ConvTranspose2d or\n",
    "        type(m) == nn.Conv2d\n",
    "       or type(m)== nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight,mode='fan_in',a=.01,nonlinearity=\"leaky_relu\")\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41781f34-547c-42a9-9eb6-0581541270c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(x,x1,x2,y1,y2,z1,z2):\n",
    "    return x[x1:x2,y1:y2,z1:z2]\n",
    "def downsample(x,factorx,factory=None,factorz=None):\n",
    "    if factory==None:\n",
    "        factory=factorx\n",
    "    if factorz==None:\n",
    "        factorz=factorx\n",
    "    return x[::factorx,::factory,::factorz]\n",
    "def brain_slice(x,slice_num):\n",
    "    return x[:,:,slice_num]\n",
    "def normalize(x,norm_type=None):\n",
    "    x-=np.min(x,keepdims=True)\n",
    "    x/=(np.max(x,keepdims=True)+.00001)\n",
    "    return x\n",
    "    \n",
    "def load_one_brain(path):\n",
    "    img=nibabel.load(path)\n",
    "    img=img.get_fdata()\n",
    "    \n",
    "    print(img.shape)\n",
    "    img=crop(img,50,178,50,178,12,140)\n",
    "    print(img.shape)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618c2512-feb7-410b-b8d2-a8ba0671849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_slice(volume,slice_num):\n",
    "    plt.imshow(volume[:,:,slice_num],cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ccfa76-eda6-4aa4-82c5-95f54ffbbeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 155)\n",
      "(128, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "basic_brain=load_one_brain(\"/usr/local/faststorage/BraTS19_Data/Training/Data/BraTS19_2013_3_1/BraTS19_2013_3_1_flair.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14615c8a-2387-4687-8c52-2ef3ff5bc983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_slice(downsample(basic_brain,2),32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91350b36-2a7c-4398-bc9b-4e0ae4ff8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim=512,k=4):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1, k, kernel_size=3, stride=1)\n",
    "        self.conv2=nn.Conv2d(k, 4*k, kernel_size=4, stride=2,padding=2)\n",
    "        self.pass_conv_1=nn.Conv2d(4*k, 16, kernel_size=3, stride=1,padding=\"same\")\n",
    "        self.conv3=nn.Conv2d(4*k, 4*k, kernel_size=3, stride=2,padding=1)\n",
    "        self.pass_conv_2=nn.Conv2d(4*k, 16, kernel_size=3, stride=1,padding=\"same\")\n",
    "        self.conv4=nn.Conv2d(4*k, 8*k, kernel_size=3, stride=2,padding=1)\n",
    "        self.conv5=nn.Conv2d(8*k, 8*k, kernel_size=3, stride=2,padding=1)\n",
    "        #self.fc1=nn.Linear(6*k*8*8,64)\n",
    "        self.z_mean_fc=nn.Linear(4*4*8*k,z_dim)\n",
    "        self.z_std_fc=nn.Linear(4*4*8*k,z_dim)\n",
    "        self.lrelu=nn.LeakyReLU()\n",
    "        self.in1=nn.BatchNorm2d(k)\n",
    "        self.in2=nn.BatchNorm2d(4*k)\n",
    "        self.in3=nn.BatchNorm2d(4*k)\n",
    "        self.in4=nn.BatchNorm2d(8*k)\n",
    "        self.Sigmoid=nn.Sigmoid()\n",
    "    def forward(self, xs):\n",
    "        x=self.lrelu(self.in1(self.conv1(xs)))\n",
    "        \n",
    "        x=self.lrelu(self.in2(self.conv2(x)))\n",
    "        \n",
    "        p1=self.pass_conv_1(x)\n",
    "        x=self.lrelu(self.in3(self.conv3(x)))\n",
    "        p2=self.pass_conv_2(x)\n",
    "        x=self.lrelu(self.in4(self.conv4(x)))\n",
    "        x=self.lrelu(self.conv5(x))\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        z_loc=self.z_mean_fc(x)\n",
    "        z_scale=torch.exp(self.z_std_fc(x))\n",
    "        return z_loc,z_scale,self.Sigmoid(p1),self.Sigmoid(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf435e8f-66c1-4eb8-b26d-0eea2076c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=256,k=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.deconv1=torch.nn.ConvTranspose2d(z_dim//4+1, 8*k, kernel_size=4, stride=2,padding=1)\n",
    "        self.deconv2=torch.nn.ConvTranspose2d(8*k, 4*k, kernel_size=4, stride=2,padding=1)\n",
    "        self.deconv3=torch.nn.ConvTranspose2d(4*k, 4*k, kernel_size=4, stride=2,padding=1)\n",
    "        self.deconv4=torch.nn.ConvTranspose2d(4*k, 2*k, kernel_size=4, stride=2,padding=1)\n",
    "        self.pass_conv1=torch.nn.Conv2d(2*k+16,2*k,kernel_size=3,stride=1,padding=\"same\")\n",
    "        self.deconv5=torch.nn.ConvTranspose2d(2*k, 2*k, kernel_size=4, stride=2,padding=1)\n",
    "        self.pass_conv2=torch.nn.Conv2d(2*k+16,2*k,kernel_size=3,stride=1,padding=\"same\")\n",
    "        self.deconv6=torch.nn.ConvTranspose2d(2*k, 2*k, kernel_size=4, stride=2,padding=1)\n",
    "        self.conv1=torch.nn.Conv2d(2*k,2,kernel_size=5,stride=1,padding=\"same\")\n",
    "        self.in1=nn.BatchNorm2d(8*k)\n",
    "        self.in2=nn.BatchNorm2d(4*k)\n",
    "        self.in3=nn.BatchNorm2d(4*k)\n",
    "        self.in4=nn.BatchNorm2d(2*k)\n",
    "        self.in5=nn.BatchNorm2d(2*k)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.lrelu=nn.LeakyReLU()\n",
    "\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.z_dim=z_dim\n",
    "    def forward(self, z):\n",
    "        z=z.view(-1,self.z_dim//4+1,2,2)\n",
    "        #print(z)\n",
    "        x = self.lrelu(self.in1(self.deconv1(z)))\n",
    "        x = self.lrelu(self.in2(self.deconv2(x)))\n",
    "        \n",
    "        x = self.lrelu(self.in3(self.deconv3(x)))\n",
    "        x = self.lrelu(self.in4(self.deconv4(x)))\n",
    "\n",
    "        #x =self.relu(self.pass_conv1(torch.cat([x,p1],1)))\n",
    "        \n",
    "        x = self.lrelu((self.deconv5(x)))\n",
    "\n",
    "        #x =self.relu(self.pass_conv2(torch.cat([x,p2],1)))\n",
    "        #x = self.lrelu(self.deconv6(x))\n",
    "        #print(x)\n",
    "        x=self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        recon_loc=x[:,0,...]\n",
    "        recon_var=torch.exp(x[:,1,...])\n",
    "        \n",
    "        \n",
    "        return recon_loc,recon_var\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b512bd02-129a-4096-9dac-b03c5108a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalAffineTransform(ConditionalTransformModule):\n",
    "    def __init__(self, context_nn, event_dim=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.event_dim = event_dim\n",
    "        self.context_nn = context_nn\n",
    "\n",
    "    def condition(self, context):\n",
    "        loc, log_scale = self.context_nn(context)\n",
    "        scale = torch.exp(log_scale)\n",
    "        ac = transforms.AffineTransform(loc, scale, event_dim=self.event_dim)\n",
    "        return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9096f1ef-777d-49eb-bae2-9db11cf27a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=1,z_dim=64,k=4):\n",
    "        super().__init__()\n",
    "        fc_1=nn.Linear(input_size,4)\n",
    "        fc_z_mean=nn.Linear(4,1)\n",
    "        fc_z_std=nn.Linear(4,1)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "    def forward(self,z,y):\n",
    "        x=self.relu(fc_1(y))\n",
    "        affine_1=fc_z_mean(x)\n",
    "        affine_2=fc_z_std(x)\n",
    "        return affine_1,affine_2\n",
    "class Estimator(nn.Module):\n",
    "    def __init__(self, input_size=512,z_dim=1,k=4):\n",
    "        super().__init__()\n",
    "        self.fc_1=nn.Linear(input_size,16)\n",
    "        self.fc_z_mean=nn.Linear(16,1)\n",
    "        self.fc_z_std=nn.Linear(16,1)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "    def forward(self,z):\n",
    "        x=self.relu(self.fc_1(z))\n",
    "        affine_1=self.fc_z_mean(x)\n",
    "        affine_2=torch.exp(self.fc_z_std(x))\n",
    "        return affine_1,affine_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26a5ef30-a3f0-4b11-9581-72a13d9ce4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, z_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(z_dim,k=32)\n",
    "        self.encoder.apply(init_weights)\n",
    "        self.decoder = Decoder(z_dim,k=32)\n",
    "        self.decoder.apply(init_weights)\n",
    "\n",
    "        #self.prior_net.train()\n",
    "        #self.generation_net.train()\n",
    "        self.z_dim=z_dim\n",
    "        #self.recognition_net = Encoder(z_dim,k=4)\n",
    "        self.age_nn=MLP(input_size=1,z_dim=4)\n",
    "        #self.age_flow_components = ConditionalAffineTransform(context_nn=age_nn, event_dim=0)\n",
    "        #self.age_flow_transforms = [self.age_flow_components]\n",
    "        self.age_estimator=Estimator(input_size=512,z_dim=1)\n",
    "    #@pyro_method\n",
    "    #def pgm_model(self):\n",
    "\n",
    "        #return {'age': age_base_dist}\n",
    "        \n",
    "    def model(self, xs,age=None):\n",
    "            # register this pytorch module and all of its sub-modules with pyro\n",
    "            \n",
    "            pyro.module(\"decoder\",self.decoder)\n",
    "            with pyro.plate(\"data\",xs.shape[0]):\n",
    "                \n",
    "                #obs=pgm_model()\n",
    "                age_mean=xs.new_ones(torch.Size((xs.shape[0], 1)))*60.8\n",
    "                age_base_scale=xs.new_ones(torch.Size((xs.shape[0], 1)))*12.89\n",
    "                age_dist = dist.Normal(age_mean, age_base_scale).to_event(1)\n",
    "                age = pyro.sample('age', age_dist)\n",
    "                z_loc = xs.new_zeros(torch.Size((xs.shape[0], self.z_dim)))\n",
    "                z_scale = z_scale = xs.new_ones(torch.Size((xs.shape[0], self.z_dim)))\n",
    "                z_base_dist = dist.Normal(z_loc, z_scale)\n",
    "                z = pyro.sample('latent',z_base_dist.to_event(1))\n",
    "                ctx=torch.cat([z,age,age,age,age],-1)  \n",
    "                #ctx=torch.cat([z],-1)  \n",
    "                \n",
    "                #RelaxedBernoulliStraightThrough\n",
    "                #z_probs_1=xs.new_ones(torch.Size((xs.shape[0], 16, 32, 32)))*.5                \n",
    "                #z_dist_1 = pyro.sample(\"z_1\",RelaxedBernoulliStraightThrough(torch.Tensor([2./3]).cuda(),probs=z_probs_1).to_event(3))\n",
    "                #z_probs_2=xs.new_ones(torch.Size((xs.shape[0], 16, 16, 16)))*.5                \n",
    "                #z_dist_2 = pyro.sample(\"z_2\",RelaxedBernoulliStraightThrough(torch.Tensor([2./3]).cuda(),probs=z_probs_2).to_event(3))\n",
    "                \n",
    "                loc,var = self.decoder(ctx)\n",
    "                #print(var)\n",
    "                #print(loc.shape)\n",
    "                pyro.sample('obs', dist.Normal(loc.unsqueeze(1),var.unsqueeze(1)).to_event(3), obs=xs)\n",
    "                #return loc\n",
    "    def guide(self, xs,age=None):\n",
    "            pyro.module(\"encoder\",self.encoder)\n",
    "            with pyro.plate(\"data\",xs.shape[0]):\n",
    "                z_loc, z_scale,z_1,z_2 = self.encoder(xs)\n",
    "                pyro.sample('latent', dist.Normal(z_loc, z_scale).to_event(1))\n",
    "                age_loc,age_scale=self.age_estimator(z_loc)\n",
    "                pyro.sample('age', dist.Normal(age_loc,age_scale).to_event(1))\n",
    "                #pyro.sample(\"z_1\",RelaxedBernoulliStraightThrough(torch.Tensor([2./3]).cuda(),probs=z_1).to_event(3))\n",
    "                #pyro.sample(\"z_2\",RelaxedBernoulliStraightThrough(torch.Tensor([2./3]).cuda(),probs=z_2).to_event(3))\n",
    "\n",
    "    def reconstruct(self,xs,age):\n",
    "        z_loc, z_scale = self.encoder(xs)\n",
    "        print(z_loc)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        ctx=torch.cat([z,age],-1)\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc,var=self.decoder(ctx)\n",
    "        return loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c9c5638c-29db-48cf-97ba-4e8ea1d05d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.cuda.current_device()\n",
    "pyro.clear_param_store()\n",
    "my_VAE=CVAE()\n",
    "my_VAE.to(device)\n",
    "#optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "optimizer = pyro.optim.Adam({\"lr\": .001})\n",
    "svi = SVI(my_VAE.model, my_VAE.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c270a875-df53-4be1-ad6e-32c586b08573",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= build_dataset(root_dir='/usr/local/faststorage/BraTS19_Data/',dataset_type=\"train\",mode=MODALS[\"T1\"])\n",
    "train_loader=VolumeLoader(dataset=train_dataset,root_dir='/usr/local/faststorage/BraTS19_Data/',dataset_type=\"train\",mode=MODALS[\"T1\"])\n",
    "train_pydl=torch.utils.data.DataLoader(train_loader,batch_size=64,num_workers=16,shuffle=False)\n",
    "\n",
    "val_dataset= build_dataset(root_dir='/usr/local/faststorage/BraTS19_Data/',dataset_type=\"val\",mode=MODALS[\"T1\"])\n",
    "val_loader=VolumeLoader(dataset=val_dataset,root_dir='/usr/local/faststorage/BraTS19_Data/',dataset_type=\"val\",mode=MODALS[\"T1\"])\n",
    "val_pydl=torch.utils.data.DataLoader(val_loader,batch_size=4,num_workers=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7ee5cd8-873d-4b1f-9061-ef46186d99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pydl):\n",
    "    loss=0.\n",
    "    for i,data in enumerate(train_pydl):\n",
    "        \n",
    "        img=data[\"MRI\"]\n",
    "        img=img[:,50:178:2,50:178:2,63]\n",
    "        img=img-torch.mean(img,axis=(1,2),keepdims=True)\n",
    "        img=img/torch.std(img,axis=(1,2),keepdims=True)\n",
    "        img=img.unsqueeze(1).type(torch.FloatTensor)\n",
    "        #for i in range(100):\n",
    "        loss+=svi.step(xs=img.cuda(),age=data[\"Age\"].type(torch.FloatTensor).view(-1,1).cuda())\n",
    "        #break\n",
    "    return loss\n",
    "def precompute(pydl):\n",
    "    loss=0.\n",
    "    ages=[]\n",
    "    for i,data in enumerate(train_pydl):\n",
    "        ages.append(data[\"Age\"].numpy())\n",
    "    print(np.mean(np.asarray(ages)))\n",
    "    print(np.std(np.asarray(ages)))\n",
    "                    \n",
    "def evaluate(pydl):\n",
    "    loss=0.\n",
    "    for i,data in enumerate(train_pydl):\n",
    "        img=data[\"MRI\"]\n",
    "        img=img[:,50:178:2,50:178:2,63]\n",
    "        img=img-torch.mean(img,axis=(1,2),keepdims=True)\n",
    "        img=img/torch.std(img,axis=(1,2),keepdims=True)\n",
    "        img=img.unsqueeze(1).type(torch.FloatTensor)\n",
    "        loss = svi.evaluate_loss(xs=img.cuda(), age=data[\"Age\"].type(torch.FloatTensor).view(-1,1).cuda())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42766a2-7c88-463e-847c-74311f5aeca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Training loss is:6907573.660545349\n",
      "Validation loss is:540079.15625\n",
      "6907573.660545349\n",
      "epoch 1\n",
      "Training loss is:2859536.6318359375\n",
      "Validation loss is:500199.44140625\n",
      "2859536.6318359375\n",
      "epoch 2\n",
      "Training loss is:2474250.858139038\n",
      "Validation loss is:470991.50146484375\n",
      "2474250.858139038\n",
      "epoch 3\n",
      "Training loss is:2280268.6011047363\n",
      "Validation loss is:431351.56689453125\n",
      "2280268.6011047363\n",
      "epoch 4\n",
      "Training loss is:3385901.3820648193\n",
      "Validation loss is:632866.046875\n",
      "epoch 5\n",
      "Training loss is:3180675.3821029663\n",
      "Validation loss is:586977.279296875\n",
      "epoch 6\n",
      "Training loss is:2973266.267303467\n",
      "Validation loss is:589658.7827148438\n",
      "epoch 7\n",
      "Training loss is:4197087.5082473755\n",
      "Validation loss is:704095.5071411133\n",
      "epoch 8\n",
      "Training loss is:3627994.942214966\n",
      "Validation loss is:735679.7260742188\n",
      "epoch 9\n",
      "Training loss is:3658476.707244873\n",
      "Validation loss is:700746.0244140625\n",
      "epoch 10\n",
      "Training loss is:3385498.7953033447\n",
      "Validation loss is:622659.4796142578\n",
      "epoch 11\n",
      "Training loss is:2981976.0676879883\n",
      "Validation loss is:548724.779296875\n",
      "epoch 12\n",
      "Training loss is:2653228.535003662\n",
      "Validation loss is:506596.51171875\n",
      "epoch 13\n",
      "Training loss is:2508361.3547439575\n",
      "Validation loss is:472652.2060546875\n",
      "epoch 14\n",
      "Training loss is:2286545.251060486\n",
      "Validation loss is:436942.3466796875\n",
      "epoch 15\n",
      "Training loss is:2158591.058654785\n",
      "Validation loss is:410264.982421875\n",
      "2158591.058654785\n",
      "epoch 16\n",
      "Training loss is:2025853.48928833\n",
      "Validation loss is:391024.8857421875\n",
      "2025853.48928833\n",
      "epoch 17\n",
      "Training loss is:1942346.9553146362\n",
      "Validation loss is:376088.8125\n",
      "1942346.9553146362\n",
      "epoch 18\n",
      "Training loss is:1871335.8403625488\n",
      "Validation loss is:363987.931640625\n",
      "1871335.8403625488\n",
      "epoch 19\n",
      "Training loss is:1810038.8241653442\n",
      "Validation loss is:356640.892578125\n",
      "1810038.8241653442\n",
      "epoch 20\n",
      "Training loss is:1768358.7766189575\n",
      "Validation loss is:348567.978515625\n",
      "1768358.7766189575\n",
      "epoch 21\n",
      "Training loss is:1729444.235786438\n",
      "Validation loss is:343056.99609375\n",
      "1729444.235786438\n",
      "epoch 22\n",
      "Training loss is:1700255.435722351\n",
      "Validation loss is:336969.3203125\n",
      "1700255.435722351\n",
      "epoch 23\n",
      "Training loss is:1674405.905303955\n",
      "Validation loss is:333029.751953125\n",
      "1674405.905303955\n",
      "epoch 24\n",
      "Training loss is:1652410.6940917969\n",
      "Validation loss is:329260.546875\n",
      "1652410.6940917969\n",
      "epoch 25\n",
      "Training loss is:1633371.008781433\n",
      "Validation loss is:325166.451171875\n",
      "1633371.008781433\n",
      "epoch 26\n",
      "Training loss is:1613761.9368972778\n",
      "Validation loss is:321819.919921875\n",
      "1613761.9368972778\n",
      "epoch 27\n",
      "Training loss is:1598330.7586746216\n",
      "Validation loss is:319008.271484375\n",
      "1598330.7586746216\n",
      "epoch 28\n",
      "Training loss is:1583577.249458313\n",
      "Validation loss is:317217.525390625\n",
      "1583577.249458313\n",
      "epoch 29\n",
      "Training loss is:1569979.9596862793\n",
      "Validation loss is:314116.818359375\n",
      "1569979.9596862793\n",
      "epoch 30\n",
      "Training loss is:1557538.5684661865\n",
      "Validation loss is:312234.193359375\n",
      "1557538.5684661865\n",
      "epoch 31\n",
      "Training loss is:1547091.8873901367\n",
      "Validation loss is:310262.20703125\n",
      "1547091.8873901367\n",
      "epoch 32\n",
      "Training loss is:1537203.1282730103\n",
      "Validation loss is:308633.29296875\n",
      "1537203.1282730103\n",
      "epoch 33\n",
      "Training loss is:1527052.70652771\n",
      "Validation loss is:306908.3046875\n",
      "1527052.70652771\n",
      "epoch 34\n",
      "Training loss is:1518870.8248062134\n",
      "Validation loss is:305131.173828125\n",
      "1518870.8248062134\n",
      "epoch 35\n",
      "Training loss is:1509294.998008728\n",
      "Validation loss is:303592.94140625\n",
      "1509294.998008728\n",
      "epoch 36\n",
      "Training loss is:1501901.3290176392\n",
      "Validation loss is:301820.41796875\n",
      "1501901.3290176392\n",
      "epoch 37\n",
      "Training loss is:1493270.384880066\n",
      "Validation loss is:300445.326171875\n",
      "1493270.384880066\n",
      "epoch 38\n",
      "Training loss is:1485819.464302063\n",
      "Validation loss is:299116.30078125\n",
      "1485819.464302063\n",
      "epoch 39\n",
      "Training loss is:1479008.1456832886\n",
      "Validation loss is:297382.39453125\n",
      "1479008.1456832886\n",
      "epoch 40\n",
      "Training loss is:1471443.40259552\n",
      "Validation loss is:296035.89453125\n",
      "1471443.40259552\n",
      "epoch 41\n",
      "Training loss is:1463507.6056518555\n",
      "Validation loss is:294584.80859375\n",
      "1463507.6056518555\n",
      "epoch 42\n",
      "Training loss is:1455056.188407898\n",
      "Validation loss is:293033.46875\n",
      "1455056.188407898\n",
      "epoch 43\n",
      "Training loss is:1446961.5045089722\n",
      "Validation loss is:291089.318359375\n",
      "1446961.5045089722\n",
      "epoch 44\n",
      "Training loss is:1437917.952796936\n",
      "Validation loss is:289397.58984375\n",
      "1437917.952796936\n",
      "epoch 45\n",
      "Training loss is:1428619.2662887573\n",
      "Validation loss is:286983.275390625\n",
      "1428619.2662887573\n",
      "epoch 46\n",
      "Training loss is:1417845.0850067139\n",
      "Validation loss is:285116.0859375\n",
      "1417845.0850067139\n",
      "epoch 47\n",
      "Training loss is:1404574.7080917358\n",
      "Validation loss is:282717.767578125\n",
      "1404574.7080917358\n",
      "epoch 48\n",
      "Training loss is:1389895.9060592651\n",
      "Validation loss is:280040.34375\n",
      "1389895.9060592651\n",
      "epoch 49\n",
      "Training loss is:1372277.0801391602\n",
      "Validation loss is:276352.904296875\n",
      "1372277.0801391602\n",
      "epoch 50\n",
      "Training loss is:1351593.8923721313\n",
      "Validation loss is:272092.544921875\n",
      "1351593.8923721313\n",
      "epoch 51\n",
      "Training loss is:1326978.74684906\n",
      "Validation loss is:267076.892578125\n",
      "1326978.74684906\n",
      "epoch 52\n",
      "Training loss is:1300081.2801742554\n",
      "Validation loss is:261919.859375\n",
      "1300081.2801742554\n",
      "epoch 53\n",
      "Training loss is:1268324.0085144043\n",
      "Validation loss is:255726.90234375\n",
      "1268324.0085144043\n",
      "epoch 54\n",
      "Training loss is:1236933.4376602173\n",
      "Validation loss is:249590.513671875\n",
      "1236933.4376602173\n",
      "epoch 55\n",
      "Training loss is:1202541.7585525513\n",
      "Validation loss is:242621.955078125\n",
      "1202541.7585525513\n",
      "epoch 56\n",
      "Training loss is:1169046.7505340576\n",
      "Validation loss is:236462.21875\n",
      "1169046.7505340576\n",
      "epoch 57\n",
      "Training loss is:1139919.6063156128\n",
      "Validation loss is:230601.3515625\n",
      "1139919.6063156128\n",
      "epoch 58\n",
      "Training loss is:1110307.4209136963\n",
      "Validation loss is:225506.220703125\n",
      "1110307.4209136963\n",
      "epoch 59\n",
      "Training loss is:1084943.4170684814\n",
      "Validation loss is:220705.87109375\n",
      "1084943.4170684814\n",
      "epoch 60\n",
      "Training loss is:1060402.5070114136\n",
      "Validation loss is:216185.34375\n",
      "1060402.5070114136\n",
      "epoch 61\n",
      "Training loss is:1038830.2854003906\n",
      "Validation loss is:211891.765625\n",
      "1038830.2854003906\n",
      "epoch 62\n",
      "Training loss is:1016570.3866653442\n",
      "Validation loss is:207961.1171875\n",
      "1016570.3866653442\n",
      "epoch 63\n",
      "Training loss is:996253.016456604\n",
      "Validation loss is:204615.66015625\n",
      "996253.016456604\n",
      "epoch 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 230, in _feed\n",
      "    close()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "<bound method _ConnectionBase.__del__ of <multiprocessing.connection.Connection object at 0x7f77fc85cb70>>Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 230, in _feed\n",
      "    close()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 132, in __del__\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 230, in _feed\n",
      "    close()\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is:976659.3360824585\n",
      "Validation loss is:201817.173828125\n",
      "976659.3360824585\n",
      "epoch 65\n",
      "Training loss is:960215.8331756592\n",
      "Validation loss is:197574.19921875\n",
      "960215.8331756592\n",
      "epoch 66\n",
      "Training loss is:942272.9835205078\n",
      "Validation loss is:195547.3203125\n",
      "942272.9835205078\n",
      "epoch 67\n",
      "Training loss is:928299.4883880615\n",
      "Validation loss is:192002.71875\n",
      "928299.4883880615\n",
      "epoch 68\n",
      "Training loss is:915133.4756317139\n",
      "Validation loss is:189562.7109375\n",
      "915133.4756317139\n",
      "epoch 69\n",
      "Training loss is:899922.5335845947\n",
      "Validation loss is:186682.638671875\n",
      "899922.5335845947\n",
      "epoch 70\n",
      "Training loss is:890277.104598999\n",
      "Validation loss is:184819.509765625\n",
      "890277.104598999\n",
      "epoch 71\n",
      "Training loss is:876843.9966125488\n",
      "Validation loss is:182856.640625\n",
      "876843.9966125488\n",
      "epoch 72\n",
      "Training loss is:867558.8143081665\n",
      "Validation loss is:180340.796875\n",
      "867558.8143081665\n",
      "epoch 73\n",
      "Training loss is:855428.2816085815\n",
      "Validation loss is:178864.451171875\n",
      "855428.2816085815\n",
      "epoch 74\n",
      "Training loss is:849681.7374420166\n",
      "Validation loss is:177354.798828125\n",
      "849681.7374420166\n",
      "epoch 75\n",
      "Training loss is:838510.6596679688\n",
      "Validation loss is:174416.638671875\n",
      "838510.6596679688\n",
      "epoch 76\n",
      "Training loss is:825150.5956039429\n",
      "Validation loss is:173642.51171875\n",
      "825150.5956039429\n",
      "epoch 77\n",
      "Training loss is:818302.2108459473\n",
      "Validation loss is:171173.837890625\n",
      "818302.2108459473\n",
      "epoch 78\n",
      "Training loss is:809124.0412368774\n",
      "Validation loss is:169361.4609375\n",
      "809124.0412368774\n",
      "epoch 79\n",
      "Training loss is:800892.6856079102\n",
      "Validation loss is:167077.470703125\n",
      "800892.6856079102\n",
      "epoch 80\n",
      "Training loss is:793490.2288818359\n",
      "Validation loss is:165998.291015625\n",
      "793490.2288818359\n",
      "epoch 81\n",
      "Training loss is:785805.0064620972\n",
      "Validation loss is:163236.568359375\n",
      "785805.0064620972\n",
      "epoch 82\n",
      "Training loss is:774400.2440795898\n",
      "Validation loss is:162184.095703125\n",
      "774400.2440795898\n",
      "epoch 83\n",
      "Training loss is:768512.7434616089\n",
      "Validation loss is:159956.2890625\n",
      "768512.7434616089\n",
      "epoch 84\n",
      "Training loss is:759642.2349853516\n",
      "Validation loss is:158103.087890625\n",
      "759642.2349853516\n",
      "epoch 85\n",
      "Training loss is:751638.5179672241\n",
      "Validation loss is:156908.822265625\n",
      "751638.5179672241\n",
      "epoch 86\n",
      "Training loss is:746092.0847320557\n",
      "Validation loss is:154301.685546875\n",
      "746092.0847320557\n",
      "epoch 87\n",
      "Training loss is:736064.0499191284\n",
      "Validation loss is:152774.287109375\n",
      "736064.0499191284\n",
      "epoch 88\n",
      "Training loss is:726167.2636489868\n",
      "Validation loss is:150450.98828125\n",
      "726167.2636489868\n",
      "epoch 89\n",
      "Training loss is:718150.049407959\n",
      "Validation loss is:148453.001953125\n",
      "718150.049407959\n",
      "epoch 90\n",
      "Training loss is:709916.356842041\n",
      "Validation loss is:146835.16796875\n",
      "709916.356842041\n",
      "epoch 91\n",
      "Training loss is:701649.9404067993\n",
      "Validation loss is:145165.875\n",
      "701649.9404067993\n",
      "epoch 92\n",
      "Training loss is:690964.8460235596\n",
      "Validation loss is:142751.302734375\n",
      "690964.8460235596\n",
      "epoch 93\n",
      "Training loss is:682241.0770263672\n",
      "Validation loss is:141333.609375\n",
      "682241.0770263672\n",
      "epoch 94\n",
      "Training loss is:676999.7070465088\n",
      "Validation loss is:140185.0625\n",
      "676999.7070465088\n",
      "epoch 95\n",
      "Training loss is:669221.5745697021\n",
      "Validation loss is:137330.3046875\n",
      "669221.5745697021\n",
      "epoch 96\n",
      "Training loss is:660545.1373901367\n",
      "Validation loss is:136332.82421875\n",
      "660545.1373901367\n",
      "epoch 97\n",
      "Training loss is:652415.1983566284\n",
      "Validation loss is:133429.162109375\n",
      "652415.1983566284\n",
      "epoch 98\n",
      "Training loss is:643581.4542312622\n",
      "Validation loss is:132425.755859375\n",
      "643581.4542312622\n",
      "epoch 99\n",
      "Training loss is:638552.2969207764\n",
      "Validation loss is:131204.873046875\n",
      "638552.2969207764\n",
      "epoch 100\n",
      "Training loss is:633441.8345336914\n",
      "Validation loss is:132644.505859375\n",
      "633441.8345336914\n",
      "epoch 101\n",
      "Training loss is:628864.6344604492\n",
      "Validation loss is:129140.65625\n",
      "628864.6344604492\n",
      "epoch 102\n",
      "Training loss is:619303.7822113037\n",
      "Validation loss is:127204.5546875\n",
      "619303.7822113037\n",
      "epoch 103\n",
      "Training loss is:606147.2393875122\n",
      "Validation loss is:125015.240234375\n",
      "606147.2393875122\n",
      "epoch 104\n",
      "Training loss is:600095.5542449951\n",
      "Validation loss is:122714.865234375\n",
      "600095.5542449951\n",
      "epoch 105\n",
      "Training loss is:588912.4457550049\n",
      "Validation loss is:121999.60546875\n",
      "588912.4457550049\n",
      "epoch 106\n",
      "Training loss is:581549.5598373413\n",
      "Validation loss is:120526.751953125\n",
      "581549.5598373413\n",
      "epoch 107\n",
      "Training loss is:577703.37915802\n",
      "Validation loss is:120283.140625\n",
      "577703.37915802\n",
      "epoch 108\n",
      "Training loss is:574263.5944061279\n",
      "Validation loss is:119531.75390625\n",
      "574263.5944061279\n",
      "epoch 109\n",
      "Training loss is:572927.5140533447\n",
      "Validation loss is:121442.291015625\n",
      "572927.5140533447\n",
      "epoch 110\n",
      "Training loss is:576747.143486023\n",
      "Validation loss is:120385.138671875\n",
      "epoch 111\n",
      "Training loss is:571542.8193054199\n",
      "Validation loss is:118423.1171875\n",
      "571542.8193054199\n",
      "epoch 112\n",
      "Training loss is:565758.9501876831\n",
      "Validation loss is:112930.609375\n",
      "565758.9501876831\n",
      "epoch 113\n",
      "Training loss is:549401.7931213379\n",
      "Validation loss is:115556.443359375\n",
      "549401.7931213379\n",
      "epoch 114\n",
      "Training loss is:538639.6644592285\n",
      "Validation loss is:110423.3828125\n",
      "538639.6644592285\n",
      "epoch 115\n",
      "Training loss is:518065.2640609741\n",
      "Validation loss is:107939.5859375\n",
      "518065.2640609741\n",
      "epoch 116\n",
      "Training loss is:504691.731338501\n",
      "Validation loss is:105020.513671875\n",
      "504691.731338501\n",
      "epoch 117\n",
      "Training loss is:496311.89124298096\n",
      "Validation loss is:103096.39453125\n",
      "496311.89124298096\n",
      "epoch 118\n",
      "Training loss is:492154.0595855713\n",
      "Validation loss is:100750.158203125\n",
      "492154.0595855713\n",
      "epoch 119\n",
      "Training loss is:488624.36880493164\n",
      "Validation loss is:99957.001953125\n",
      "488624.36880493164\n",
      "epoch 120\n",
      "Training loss is:478925.38973999023\n",
      "Validation loss is:97663.7421875\n",
      "478925.38973999023\n",
      "epoch 121\n",
      "Training loss is:485462.4775772095\n",
      "Validation loss is:105735.845703125\n",
      "epoch 122\n",
      "Training loss is:477842.94107818604\n",
      "Validation loss is:96827.06640625\n",
      "477842.94107818604\n",
      "epoch 123\n",
      "Training loss is:459990.79246520996\n",
      "Validation loss is:95108.591796875\n",
      "459990.79246520996\n",
      "epoch 124\n",
      "Training loss is:587461.138343811\n",
      "Validation loss is:139477.82421875\n",
      "epoch 125\n",
      "Training loss is:610878.7619781494\n",
      "Validation loss is:135043.6796875\n",
      "epoch 126\n",
      "Training loss is:622655.1180648804\n",
      "Validation loss is:120557.64453125\n",
      "epoch 127\n",
      "Training loss is:573675.4756774902\n",
      "Validation loss is:113973.623046875\n",
      "epoch 128\n",
      "Training loss is:527642.4237136841\n",
      "Validation loss is:105572.73828125\n",
      "epoch 129\n",
      "Training loss is:498133.8143005371\n",
      "Validation loss is:100634.87109375\n",
      "epoch 130\n",
      "Training loss is:470016.99183654785\n",
      "Validation loss is:96037.1484375\n",
      "epoch 131\n",
      "Training loss is:455150.79009246826\n",
      "Validation loss is:98214.494140625\n",
      "455150.79009246826\n",
      "epoch 132\n",
      "Training loss is:497185.4144439697\n",
      "Validation loss is:100597.78515625\n",
      "epoch 133\n",
      "Training loss is:498588.64879608154\n",
      "Validation loss is:111288.20703125\n",
      "epoch 134\n",
      "Training loss is:518572.9451904297\n",
      "Validation loss is:109523.552734375\n",
      "epoch 135\n",
      "Training loss is:488559.9203491211\n",
      "Validation loss is:99321.52734375\n",
      "epoch 136\n",
      "Training loss is:466859.7286682129\n",
      "Validation loss is:92162.328125\n",
      "epoch 137\n",
      "Training loss is:442468.2674407959\n",
      "Validation loss is:91924.212890625\n",
      "442468.2674407959\n",
      "epoch 138\n",
      "Training loss is:418753.9133300781\n",
      "Validation loss is:86406.97265625\n",
      "418753.9133300781\n",
      "epoch 139\n",
      "Training loss is:401992.2814254761\n",
      "Validation loss is:82310.400390625\n",
      "401992.2814254761\n",
      "epoch 140\n",
      "Training loss is:399254.41960144043\n",
      "Validation loss is:82752.169921875\n",
      "399254.41960144043\n",
      "epoch 141\n",
      "Training loss is:384241.6352996826\n",
      "Validation loss is:80630.9765625\n",
      "384241.6352996826\n",
      "epoch 142\n",
      "Training loss is:386462.5051956177\n",
      "Validation loss is:86109.279296875\n",
      "epoch 143\n",
      "Training loss is:398420.4647369385\n",
      "Validation loss is:87560.21484375\n",
      "epoch 144\n",
      "Training loss is:458299.40461730957\n",
      "Validation loss is:107828.5390625\n",
      "epoch 145\n",
      "Training loss is:530711.3478012085\n",
      "Validation loss is:103909.646484375\n",
      "epoch 146\n",
      "Training loss is:489460.7456359863\n",
      "Validation loss is:104654.2109375\n",
      "epoch 147\n",
      "Training loss is:480556.62226104736\n",
      "Validation loss is:100943.53125\n",
      "epoch 148\n",
      "Training loss is:445265.8423538208\n",
      "Validation loss is:89960.64453125\n",
      "epoch 149\n",
      "Training loss is:431233.7162628174\n",
      "Validation loss is:90043.03515625\n",
      "epoch 150\n",
      "Training loss is:414338.0891342163\n",
      "Validation loss is:84612.09375\n",
      "epoch 151\n",
      "Training loss is:383040.79528045654\n",
      "Validation loss is:82389.205078125\n",
      "383040.79528045654\n",
      "epoch 152\n",
      "Training loss is:399071.3634414673\n",
      "Validation loss is:91987.302734375\n",
      "epoch 153\n",
      "Training loss is:453080.1658554077\n",
      "Validation loss is:107452.4921875\n",
      "epoch 154\n",
      "Training loss is:487654.5996246338\n",
      "Validation loss is:98282.611328125\n",
      "epoch 155\n",
      "Training loss is:453020.8446884155\n",
      "Validation loss is:92929.078125\n",
      "epoch 156\n",
      "Training loss is:429329.8949661255\n",
      "Validation loss is:87650.177734375\n",
      "epoch 157\n",
      "Training loss is:401817.8220062256\n",
      "Validation loss is:78711.9453125\n",
      "epoch 158\n",
      "Training loss is:371433.70349884033\n",
      "Validation loss is:77472.828125\n",
      "371433.70349884033\n",
      "epoch 159\n",
      "Training loss is:365286.36724853516\n",
      "Validation loss is:76511.837890625\n",
      "365286.36724853516\n",
      "epoch 160\n",
      "Training loss is:388772.87059020996\n",
      "Validation loss is:96217.28125\n",
      "epoch 161\n",
      "Training loss is:404484.6224975586\n",
      "Validation loss is:74937.9765625\n",
      "epoch 162\n",
      "Training loss is:395352.326423645\n",
      "Validation loss is:88280.943359375\n",
      "epoch 163\n",
      "Training loss is:403191.2212677002\n",
      "Validation loss is:80697.6171875\n",
      "epoch 164\n",
      "Training loss is:362988.9397125244\n",
      "Validation loss is:72994.08203125\n",
      "362988.9397125244\n",
      "epoch 165\n",
      "Training loss is:355605.127822876\n",
      "Validation loss is:79135.306640625\n",
      "355605.127822876\n",
      "epoch 166\n",
      "Training loss is:355006.03552246094\n",
      "Validation loss is:73686.287109375\n",
      "355006.03552246094\n",
      "epoch 167\n",
      "Training loss is:325705.5898132324\n",
      "Validation loss is:67861.251953125\n",
      "325705.5898132324\n",
      "epoch 168\n",
      "Training loss is:303507.3878326416\n",
      "Validation loss is:64457.49609375\n",
      "303507.3878326416\n",
      "epoch 169\n",
      "Training loss is:354400.5138015747\n",
      "Validation loss is:75606.65625\n",
      "epoch 170\n",
      "Training loss is:361090.5439529419\n",
      "Validation loss is:86212.4375\n",
      "epoch 171\n",
      "Training loss is:394384.32107543945\n",
      "Validation loss is:76898.873046875\n",
      "epoch 172\n",
      "Training loss is:361448.607421875\n",
      "Validation loss is:76596.650390625\n",
      "epoch 173\n",
      "Training loss is:335137.46518707275\n",
      "Validation loss is:64712.96484375\n",
      "epoch 174\n",
      "Training loss is:298922.9409713745\n",
      "Validation loss is:61797.138671875\n",
      "298922.9409713745\n",
      "epoch 175\n",
      "Training loss is:280071.57221221924\n",
      "Validation loss is:60175.115234375\n",
      "280071.57221221924\n",
      "epoch 176\n",
      "Training loss is:284389.1335144043\n",
      "Validation loss is:69116.072265625\n",
      "epoch 177\n",
      "Training loss is:379363.2912597656\n",
      "Validation loss is:102314.3359375\n",
      "epoch 178\n",
      "Training loss is:475512.98345184326\n",
      "Validation loss is:103998.4609375\n",
      "epoch 179\n",
      "Training loss is:453455.5842666626\n",
      "Validation loss is:93994.4453125\n",
      "epoch 180\n",
      "Training loss is:437466.20513153076\n",
      "Validation loss is:82107.71875\n",
      "epoch 181\n",
      "Training loss is:389605.1195449829\n",
      "Validation loss is:75369.798828125\n",
      "epoch 182\n",
      "Training loss is:344502.24784088135\n",
      "Validation loss is:67408.845703125\n",
      "epoch 183\n",
      "Training loss is:307955.6710739136\n",
      "Validation loss is:63216.12890625\n",
      "epoch 184\n",
      "Training loss is:292962.91176605225\n",
      "Validation loss is:58677.2734375\n",
      "epoch 185\n",
      "Training loss is:285618.33575439453\n",
      "Validation loss is:63232.857421875\n",
      "epoch 186\n",
      "Training loss is:275509.24504089355\n",
      "Validation loss is:59753.94140625\n",
      "275509.24504089355\n",
      "epoch 187\n",
      "Training loss is:265518.3197402954\n",
      "Validation loss is:63606.064453125\n",
      "265518.3197402954\n",
      "epoch 188\n",
      "Training loss is:266868.13970947266\n",
      "Validation loss is:59358.72265625\n",
      "epoch 189\n",
      "Training loss is:289654.99800109863\n",
      "Validation loss is:73613.423828125\n",
      "epoch 190\n",
      "Training loss is:400300.9713973999\n",
      "Validation loss is:134286.5478515625\n",
      "epoch 191\n",
      "Training loss is:621525.6946334839\n",
      "Validation loss is:126449.81640625\n",
      "epoch 192\n",
      "Training loss is:608494.7517166138\n",
      "Validation loss is:115615.552734375\n",
      "epoch 193\n",
      "Training loss is:530346.9472503662\n",
      "Validation loss is:99283.021484375\n",
      "epoch 194\n",
      "Training loss is:477949.30924987793\n",
      "Validation loss is:94933.1015625\n",
      "epoch 195\n",
      "Training loss is:436373.32453918457\n",
      "Validation loss is:84471.265625\n",
      "epoch 196\n",
      "Training loss is:386921.1063308716\n",
      "Validation loss is:76655.427734375\n",
      "epoch 197\n",
      "Training loss is:349949.4248123169\n",
      "Validation loss is:69451.921875\n",
      "epoch 198\n",
      "Training loss is:308651.2741317749\n",
      "Validation loss is:63149.025390625\n",
      "epoch 199\n",
      "Training loss is:288335.3914337158\n",
      "Validation loss is:58173.1015625\n",
      "epoch 200\n",
      "Training loss is:258656.78326416016\n",
      "Validation loss is:53859.248046875\n",
      "258656.78326416016\n",
      "epoch 201\n",
      "Training loss is:244744.40506744385\n",
      "Validation loss is:50797.890625\n",
      "244744.40506744385\n",
      "epoch 202\n",
      "Training loss is:235230.30673217773\n",
      "Validation loss is:52363.57421875\n",
      "235230.30673217773\n",
      "epoch 203\n",
      "Training loss is:217968.2165222168\n",
      "Validation loss is:47816.138671875\n",
      "217968.2165222168\n",
      "epoch 204\n",
      "Training loss is:220524.29415893555\n",
      "Validation loss is:44294.462890625\n",
      "epoch 205\n",
      "Training loss is:331822.3331756592\n",
      "Validation loss is:88643.740234375\n",
      "epoch 206\n",
      "Training loss is:333471.84063720703\n",
      "Validation loss is:76293.931640625\n",
      "epoch 207\n",
      "Training loss is:345445.1277618408\n",
      "Validation loss is:74583.6171875\n",
      "epoch 208\n",
      "Training loss is:316406.498336792\n",
      "Validation loss is:67747.28125\n",
      "epoch 209\n",
      "Training loss is:300276.6010131836\n",
      "Validation loss is:61549.205078125\n",
      "epoch 210\n",
      "Training loss is:262365.980506897\n",
      "Validation loss is:55775.177734375\n",
      "epoch 211\n",
      "Training loss is:255995.80535888672\n",
      "Validation loss is:53333.98046875\n",
      "epoch 212\n",
      "Training loss is:237572.167137146\n",
      "Validation loss is:53465.734375\n",
      "epoch 213\n",
      "Training loss is:231976.21559906006\n",
      "Validation loss is:53375.357421875\n",
      "epoch 214\n",
      "Training loss is:227380.60636901855\n",
      "Validation loss is:51296.04296875\n",
      "epoch 215\n",
      "Training loss is:211593.96531677246\n",
      "Validation loss is:49027.169921875\n",
      "211593.96531677246\n",
      "epoch 216\n",
      "Training loss is:202526.42575073242\n",
      "Validation loss is:54786.314453125\n",
      "202526.42575073242\n",
      "epoch 217\n",
      "Training loss is:226220.9508972168\n",
      "Validation loss is:60328.845703125\n",
      "epoch 218\n",
      "Training loss is:266902.28118896484\n",
      "Validation loss is:71673.919921875\n",
      "epoch 219\n",
      "Training loss is:393708.64318084717\n",
      "Validation loss is:98476.6328125\n",
      "epoch 220\n",
      "Training loss is:422959.2960128784\n",
      "Validation loss is:94921.64453125\n",
      "epoch 221\n",
      "Training loss is:423586.8679885864\n",
      "Validation loss is:77951.423828125\n",
      "epoch 222\n",
      "Training loss is:367547.0668334961\n",
      "Validation loss is:75401.654296875\n",
      "epoch 223\n",
      "Training loss is:340965.22762298584\n",
      "Validation loss is:63659.095703125\n",
      "epoch 224\n",
      "Training loss is:291607.20499420166\n",
      "Validation loss is:56445.041015625\n",
      "epoch 225\n",
      "Training loss is:247214.34456634521\n",
      "Validation loss is:51841.4453125\n",
      "epoch 226\n",
      "Training loss is:227774.92993927002\n",
      "Validation loss is:50151.228515625\n",
      "epoch 227\n",
      "Training loss is:213914.0644454956\n",
      "Validation loss is:45291.349609375\n",
      "epoch 228\n",
      "Training loss is:234662.5630493164\n",
      "Validation loss is:54660.759765625\n",
      "epoch 229\n",
      "Training loss is:243450.52661895752\n",
      "Validation loss is:68225.857421875\n",
      "epoch 230\n",
      "Training loss is:269150.868309021\n",
      "Validation loss is:52586.595703125\n",
      "epoch 231\n",
      "Training loss is:250542.67694854736\n",
      "Validation loss is:49503.5859375\n",
      "epoch 232\n",
      "Training loss is:219363.81405639648\n",
      "Validation loss is:45886.12890625\n",
      "epoch 233\n",
      "Training loss is:202988.1561126709\n",
      "Validation loss is:50177.892578125\n",
      "epoch 234\n",
      "Training loss is:205955.12091064453\n",
      "Validation loss is:45504.740234375\n",
      "epoch 235\n",
      "Training loss is:185058.35597991943\n",
      "Validation loss is:44423.189453125\n",
      "185058.35597991943\n",
      "epoch 236\n",
      "Training loss is:181835.59368133545\n",
      "Validation loss is:40330.4921875\n",
      "181835.59368133545\n",
      "epoch 237\n",
      "Training loss is:167629.69286346436\n",
      "Validation loss is:35647.494140625\n",
      "167629.69286346436\n",
      "epoch 238\n",
      "Training loss is:259051.2299041748\n",
      "Validation loss is:57710.1953125\n",
      "epoch 239\n",
      "Training loss is:232104.74340820312\n",
      "Validation loss is:68524.9140625\n",
      "epoch 240\n",
      "Training loss is:309876.0771408081\n",
      "Validation loss is:68447.927734375\n",
      "epoch 241\n",
      "Training loss is:264978.0372161865\n",
      "Validation loss is:56521.181640625\n",
      "epoch 242\n",
      "Training loss is:249223.65579986572\n",
      "Validation loss is:51777.455078125\n",
      "epoch 243\n",
      "Training loss is:219167.93824768066\n",
      "Validation loss is:44503.03515625\n",
      "epoch 244\n",
      "Training loss is:202967.96607208252\n",
      "Validation loss is:42274.689453125\n",
      "epoch 245\n",
      "Training loss is:167533.30764007568\n",
      "Validation loss is:36667.154296875\n",
      "167533.30764007568\n",
      "epoch 246\n",
      "Training loss is:149047.95602416992\n",
      "Validation loss is:33526.6328125\n",
      "149047.95602416992\n",
      "epoch 247\n",
      "Training loss is:153241.95280456543\n",
      "Validation loss is:30343.5546875\n",
      "epoch 248\n",
      "Training loss is:157721.88861083984\n",
      "Validation loss is:33131.609375\n",
      "epoch 249\n",
      "Training loss is:166596.76847839355\n",
      "Validation loss is:29435.0146484375\n",
      "epoch 250\n",
      "Training loss is:282981.8440170288\n",
      "Validation loss is:132381.3291015625\n",
      "epoch 251\n",
      "Training loss is:572140.9158401489\n",
      "Validation loss is:99527.58984375\n",
      "epoch 252\n",
      "Training loss is:501265.1272125244\n",
      "Validation loss is:101550.162109375\n",
      "epoch 253\n",
      "Training loss is:460401.5719528198\n",
      "Validation loss is:88868.51171875\n",
      "epoch 254\n",
      "Training loss is:402695.63858795166\n",
      "Validation loss is:78337.08203125\n",
      "epoch 255\n",
      "Training loss is:368473.75269317627\n",
      "Validation loss is:69239.732421875\n",
      "epoch 256\n",
      "Training loss is:325660.2530288696\n",
      "Validation loss is:64214.73046875\n",
      "epoch 257\n",
      "Training loss is:283685.2323226929\n",
      "Validation loss is:57175.96484375\n",
      "epoch 258\n",
      "Training loss is:249013.47101593018\n",
      "Validation loss is:50492.537109375\n",
      "epoch 259\n",
      "Training loss is:213722.0715866089\n",
      "Validation loss is:43299.80859375\n",
      "epoch 260\n",
      "Training loss is:200020.16885375977\n",
      "Validation loss is:40497.73046875\n",
      "epoch 261\n",
      "Training loss is:177875.17924499512\n",
      "Validation loss is:38341.47265625\n",
      "epoch 262\n",
      "Training loss is:148062.419380188\n",
      "Validation loss is:30686.236328125\n",
      "148062.419380188\n",
      "epoch 263\n",
      "Training loss is:128142.74243164062\n",
      "Validation loss is:28961.154296875\n",
      "128142.74243164062\n",
      "epoch 264\n",
      "Training loss is:119892.41076660156\n",
      "Validation loss is:26910.15625\n",
      "119892.41076660156\n",
      "epoch 265\n",
      "Training loss is:110252.81025695801\n",
      "Validation loss is:26069.4140625\n",
      "110252.81025695801\n",
      "epoch 266\n",
      "Training loss is:94925.42951202393\n",
      "Validation loss is:21989.533203125\n",
      "94925.42951202393\n",
      "epoch 267\n",
      "Training loss is:155124.89602661133\n",
      "Validation loss is:31914.67578125\n",
      "epoch 268\n",
      "Training loss is:130281.44582366943\n",
      "Validation loss is:34532.470703125\n",
      "epoch 269\n",
      "Training loss is:191129.2484817505\n",
      "Validation loss is:57830.54296875\n",
      "epoch 270\n",
      "Training loss is:254593.8748474121\n",
      "Validation loss is:55107.0234375\n",
      "epoch 271\n",
      "Training loss is:222702.1058731079\n",
      "Validation loss is:43164.576171875\n",
      "epoch 272\n",
      "Training loss is:190089.5635986328\n",
      "Validation loss is:45795.23046875\n",
      "epoch 273\n",
      "Training loss is:175872.24365234375\n",
      "Validation loss is:35587.849609375\n",
      "epoch 274\n",
      "Training loss is:139447.27487945557\n",
      "Validation loss is:27258.109375\n",
      "epoch 275\n",
      "Training loss is:118389.42561340332\n",
      "Validation loss is:27662.955078125\n",
      "epoch 276\n",
      "Training loss is:110900.05303955078\n",
      "Validation loss is:24197.4140625\n",
      "epoch 277\n",
      "Training loss is:87563.19204711914\n",
      "Validation loss is:18534.591796875\n",
      "87563.19204711914\n",
      "epoch 278\n",
      "Training loss is:94743.39891052246\n",
      "Validation loss is:20157.0390625\n",
      "epoch 279\n",
      "Training loss is:93188.8189086914\n",
      "Validation loss is:38256.705078125\n",
      "epoch 280\n",
      "Training loss is:188636.4108657837\n",
      "Validation loss is:30700.3515625\n",
      "epoch 281\n",
      "Training loss is:152944.70782470703\n",
      "Validation loss is:34282.89453125\n",
      "epoch 282\n",
      "Training loss is:195820.21619415283\n",
      "Validation loss is:43468.443359375\n",
      "epoch 283\n",
      "Training loss is:165956.83228302002\n",
      "Validation loss is:34319.033203125\n",
      "epoch 284\n",
      "Training loss is:149402.79844665527\n",
      "Validation loss is:40391.904296875\n",
      "epoch 285\n",
      "Training loss is:161819.53276062012\n",
      "Validation loss is:27651.466796875\n",
      "epoch 286\n",
      "Training loss is:109569.63377380371\n",
      "Validation loss is:20840.48046875\n",
      "epoch 287\n",
      "Training loss is:86912.61595153809\n",
      "Validation loss is:20079.599609375\n",
      "86912.61595153809\n",
      "epoch 288\n",
      "Training loss is:87259.39482116699\n",
      "Validation loss is:25105.412109375\n",
      "epoch 289\n",
      "Training loss is:64231.959480285645\n",
      "Validation loss is:16773.103515625\n",
      "64231.959480285645\n",
      "epoch 290\n",
      "Training loss is:71298.97780609131\n",
      "Validation loss is:22125.869140625\n",
      "epoch 291\n",
      "Training loss is:210271.30488586426\n",
      "Validation loss is:31147.173828125\n",
      "epoch 292\n",
      "Training loss is:305807.55473327637\n",
      "Validation loss is:61541.3349609375\n",
      "epoch 293\n",
      "Training loss is:311544.993560791\n",
      "Validation loss is:62941.0546875\n",
      "epoch 294\n",
      "Training loss is:298503.52112579346\n",
      "Validation loss is:61005.61328125\n",
      "epoch 295\n",
      "Training loss is:264137.96745300293\n",
      "Validation loss is:56376.986328125\n",
      "epoch 296\n",
      "Training loss is:219338.18044281006\n",
      "Validation loss is:45225.76171875\n",
      "epoch 297\n",
      "Training loss is:184892.2317276001\n",
      "Validation loss is:40413.72265625\n",
      "epoch 298\n",
      "Training loss is:149082.47495269775\n",
      "Validation loss is:31187.544921875\n",
      "epoch 299\n",
      "Training loss is:119500.5308227539\n",
      "Validation loss is:27899.361328125\n",
      "epoch 300\n",
      "Training loss is:97101.89401245117\n",
      "Validation loss is:21066.197265625\n",
      "epoch 301\n",
      "Training loss is:90465.66148376465\n",
      "Validation loss is:25595.119140625\n",
      "epoch 302\n",
      "Training loss is:99815.84336853027\n",
      "Validation loss is:29717.275390625\n",
      "epoch 303\n",
      "Training loss is:90436.96486663818\n",
      "Validation loss is:23414.0234375\n",
      "epoch 304\n",
      "Training loss is:132476.0523071289\n",
      "Validation loss is:19891.705078125\n",
      "epoch 305\n",
      "Training loss is:246091.54419708252\n",
      "Validation loss is:98629.5830078125\n",
      "epoch 306\n",
      "Training loss is:422325.52251434326\n",
      "Validation loss is:74364.580078125\n",
      "epoch 307\n",
      "Training loss is:364327.6283798218\n",
      "Validation loss is:65377.92578125\n",
      "epoch 308\n",
      "Training loss is:299110.84492492676\n",
      "Validation loss is:65074.384765625\n",
      "epoch 309\n",
      "Training loss is:266375.7779083252\n",
      "Validation loss is:49035.830078125\n",
      "epoch 310\n",
      "Training loss is:213980.87591552734\n",
      "Validation loss is:43459.76171875\n",
      "epoch 311\n",
      "Training loss is:184225.87534332275\n",
      "Validation loss is:35082.94921875\n",
      "epoch 312\n",
      "Training loss is:140435.55695343018\n",
      "Validation loss is:27939.904296875\n",
      "epoch 313\n",
      "Training loss is:106032.17261505127\n",
      "Validation loss is:21717.326171875\n",
      "epoch 314\n",
      "Training loss is:79975.89498138428\n",
      "Validation loss is:16354.109375\n",
      "epoch 315\n",
      "Training loss is:53110.096282958984\n",
      "Validation loss is:11853.490234375\n",
      "53110.096282958984\n",
      "epoch 316\n",
      "Training loss is:36118.10890960693\n",
      "Validation loss is:11483.091796875\n",
      "36118.10890960693\n",
      "epoch 317\n",
      "Training loss is:21773.949989318848\n",
      "Validation loss is:7834.984375\n",
      "21773.949989318848\n",
      "epoch 318\n",
      "Training loss is:62433.38523101807\n",
      "Validation loss is:44468.9033203125\n",
      "epoch 319\n",
      "Training loss is:74743.8254776001\n",
      "Validation loss is:15375.7451171875\n",
      "epoch 320\n",
      "Training loss is:69288.9825515747\n",
      "Validation loss is:28941.525390625\n",
      "epoch 321\n",
      "Training loss is:235245.65126800537\n",
      "Validation loss is:34787.4140625\n",
      "epoch 322\n",
      "Training loss is:192551.7441253662\n",
      "Validation loss is:37925.103515625\n",
      "epoch 323\n",
      "Training loss is:174902.66762542725\n",
      "Validation loss is:32674.765625\n",
      "epoch 324\n",
      "Training loss is:140449.40895080566\n",
      "Validation loss is:28917.306640625\n",
      "epoch 325\n",
      "Training loss is:106041.78659820557\n",
      "Validation loss is:21865.611328125\n",
      "epoch 326\n",
      "Training loss is:80109.59186553955\n",
      "Validation loss is:16850.99609375\n",
      "epoch 327\n",
      "Training loss is:54500.0546875\n",
      "Validation loss is:8862.9140625\n",
      "epoch 328\n",
      "Training loss is:54003.11095428467\n",
      "Validation loss is:8639.2109375\n",
      "epoch 329\n",
      "Training loss is:77643.64431762695\n",
      "Validation loss is:8136.234375\n",
      "epoch 330\n",
      "Training loss is:154461.87547302246\n",
      "Validation loss is:91723.111328125\n",
      "epoch 331\n",
      "Training loss is:379109.05993652344\n",
      "Validation loss is:83672.80078125\n",
      "epoch 332\n",
      "Training loss is:533758.5886154175\n",
      "Validation loss is:157931.865234375\n",
      "epoch 333\n",
      "Training loss is:839060.4605712891\n",
      "Validation loss is:169028.8359375\n",
      "epoch 334\n",
      "Training loss is:775502.6963729858\n",
      "Validation loss is:135896.81640625\n",
      "epoch 335\n",
      "Training loss is:594678.5249328613\n",
      "Validation loss is:110157.880859375\n",
      "epoch 336\n",
      "Training loss is:502357.3162918091\n",
      "Validation loss is:94552.115234375\n",
      "epoch 337\n",
      "Training loss is:416201.9746170044\n",
      "Validation loss is:77433.388671875\n",
      "epoch 338\n",
      "Training loss is:353890.45066833496\n",
      "Validation loss is:69511.00390625\n",
      "epoch 339\n",
      "Training loss is:306045.33879852295\n",
      "Validation loss is:59437.6953125\n",
      "epoch 340\n",
      "Training loss is:259887.3489074707\n",
      "Validation loss is:52268.94921875\n",
      "epoch 341\n",
      "Training loss is:221559.90077209473\n",
      "Validation loss is:44333.833984375\n",
      "epoch 342\n",
      "Training loss is:186620.01735687256\n",
      "Validation loss is:38254.763671875\n",
      "epoch 343\n",
      "Training loss is:164748.3765411377\n",
      "Validation loss is:35613.798828125\n",
      "epoch 344\n",
      "Training loss is:169994.29484558105\n",
      "Validation loss is:45400.603515625\n",
      "epoch 345\n",
      "Training loss is:219549.8876876831\n",
      "Validation loss is:51899.8515625\n",
      "epoch 346\n",
      "Training loss is:259763.37413024902\n",
      "Validation loss is:64883.3193359375\n",
      "epoch 347\n",
      "Training loss is:251210.59665679932\n",
      "Validation loss is:57011.7265625\n",
      "epoch 348\n",
      "Training loss is:228161.7617263794\n",
      "Validation loss is:48042.9453125\n",
      "epoch 349\n",
      "Training loss is:220212.37641906738\n",
      "Validation loss is:39046.13671875\n",
      "epoch 350\n",
      "Training loss is:170374.47966766357\n",
      "Validation loss is:36672.966796875\n",
      "epoch 351\n",
      "Training loss is:139795.96758270264\n",
      "Validation loss is:31324.453125\n",
      "epoch 352\n",
      "Training loss is:119464.97780609131\n",
      "Validation loss is:23619.16015625\n",
      "epoch 353\n",
      "Training loss is:89891.83504486084\n",
      "Validation loss is:22848.07421875\n",
      "epoch 354\n",
      "Training loss is:95991.81392669678\n",
      "Validation loss is:21399.703125\n",
      "epoch 355\n",
      "Training loss is:77091.27628326416\n",
      "Validation loss is:20666.6171875\n",
      "epoch 356\n"
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]\n",
    "val_loss_best=np.inf\n",
    "#precompute(train_pydl)\n",
    "for i in range(1000):\n",
    "    print(\"epoch \" +str(i))\n",
    "    train_loss.append(train(train_pydl))\n",
    "    print(\"Training loss is:\" +str(train_loss[-1]))\n",
    "    val_loss.append(evaluate(val_pydl))\n",
    "    print(\"Validation loss is:\" +str(val_loss[-1]))\n",
    "    if train_loss[-1]<val_loss_best:\n",
    "        val_loss_best=train_loss[-1]\n",
    "        print(val_loss_best)\n",
    "        torch.save(my_VAE, \"./best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc63463-b18b-46d3-b41f-d22e76a03e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cea5ce-f6da-4dda-9443-9ecea60bafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(my_VAE.reconstruct(torch.Tensor(resha_slice),age=torch.Tensor([[50]]).view(1,1)).detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce574474-4db4-4c7e-9a89-f2eff76e93e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d8a52-fac9-46a5-a68b-964c5be483e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfb585-ed0f-4e48-9bf5-57ae7d9a1b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bf9b0-64d6-496a-a767-f2d35b192aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5babd3-81e4-4e02-a0df-df4755e96229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee962a-8abb-4544-8694-a661f57a9eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b600e27-9512-4681-b529-7308981df766",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea4103-2a5a-4bfe-9c78-05bc045c2587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccba09-9379-495d-940d-f969936010c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0525d00b-f71d-4879-8247-be18f4d956e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
